## Advantages of machine transcriptions, intro to OCR, challenges

Human transcription is expensive and enormously time-consuming: it requires a trained paleographer and hours of labor per manuscript. Machines could produce transcriptions quickly and en-masse, saving time and money and flooding the field with new digital texts. With more transcriptions we could compare more manuscripts across a single textual tradition or discover more about lesser-known texts. Transcriptions could be checked and edited for formal digital and print editions, but even a database of purely automatic transcriptions would be useful for many scholars, who could read and search across the texts for relevant information and then double check digital facsimiles when necessary. Researchers could search precisely within digital facsimiles and read full transcriptions alongside the the virtual folios. Digital transcriptions could be used as datasets for large-scale data analysis (such as topic modeling and text mining). Finally, machines might eventually produce more accurate transcriptions than humans, or in the very least they might be less prone to making the kinds of transcription errors that humans make (we will discuss this possibility in further detail below).

Optical Character Recognition (OCR) is a method of turning print or handwriting into digital text and can allow researchers to automatically convert images of manuscripts into digital transcriptions. OCR transforms a text-image—for example a scanned book or a photograph of a document—into machine-readable text which can then be digitally searched, edited, stored, and otherwise manipulated by a computer. OCR technology already underlies many of the digital texts we read: it is what enables Project Gutenberg to easily digitize printed texts and Google Books to perform keyword-searches on scanned books. OCR models are built through machine learning, meaning that the machine executes tasks through inferred rules rather than explicit instruction. The machine learns these inferred rules through a "training" process, wherein a researcher provides the machine with a "training set" of paired inputs and outputs. Then machine instructs itself on this training set: it generates a model which, given a random input, guesses an output, it compares the guess to the target output, it adjusts the model accordingly, and it repeats this process until the model guesses outputs as accurately as possible without "overfitting".\[insert footnote: How exactly the machine adjusts its model depends on the learning algorithm used. There is also a second step to model creation, wherein a separate validation dataset evaluates the model and fine-tunes it but isn't used to "teach" the model. After a certain point in the training process, the model will start to "overfit," which means its predictions are too dependent on the training set to produce reliable predictions of a test test, so training can't simply be run ad infinitum to increase accuracy.\] In the case of manuscript transcription, a machine is first given a training set composed of manuscript images (input) alongside corresponding digital transcriptions (output); the machine next “trains” on the images and transcriptions and “learns” the transcription rules on its own through guessing and correcting. The machine can then produce its own transcriptions on new, previously unseen images.

Medieval manuscripts pose significant challenges to automated processing. Unlike printed texts, medieval handwriting often contains non-discrete characters, such as in the conjoined letters of cursive scripts or the disconnected minims of Gothic script; therefore machines cannot simply be taught individual letter forms, but must learn to transcribe larger segments of text [does the vatican project challenge this idea?]. Compared to modern handwritten documents, medieval manuscripts feature elaborate and often cryptic handwriting systems which vary intensely across period, region, and scribe. Accordingly, distinct OCR systems must be trained on distinct medieval scripts, which means compiling training data on tens, if not hundreds, of manuscripts per script. Furthermore, medieval manuscripts often have extensive marginal notes, decorations, and damage, making it difficult to distinguish text from other information in a digital image. Finally, given that the most advanced OCR engines often rely on comprehensive dictionaries and natural language processing to improve accuracy, the inconsistent orthography of medieval languages (like Middle English or Old French) poses an additional obstacle to efficient automated transcription.

When we first began this project, we knew of no team successfully using OCR to transcribe medieval manuscripts. Recently several teams have published results similar to our own, and in some cases have achieved even higher accuracy rates. Moreover the potential for automatic transcription has been firmly established over the past year, and our aim is rather to discuss certain decisions, methods, and implications of the technology.  

\[footnote]\Projects like the Piers Plowman Electronic Archive, which has produced digital transcriptions of eight B-Text manuscripts, attest to the usefulness of an easily accessible, readable, and usable digital archive. However,

---
notes

human error is variable and unpredictable, whereas machine error is more likelier to be consistent, and when identified, systematically correctable.
