Optical Character Recognition (OCR) transforms a text-image—for example a scanned book or a photograph of a document—into machine-readable text. This conversion process allows print and handwritten texts to be digitally searched, edited, stored, and otherwise manipulated by a computer. OCR technology already underlies many of the digital texts we read: it is what enables Project Gutenberg to easily digitize printed texts and Google Books to perform keyword-searches on scanned books. OCR models are built through machine learning, meaning that the machine executes tasks through inferred rules rather than explicit instruction. The machine learns these inferred rules through a "training" process, wherein a researcher provides the machine with a set of matched inputs and outputs; the machine then guesses an output for a given input, compares its guess to the real output, and repeats the process many times over until it eventually learns rules to make better guesses. In the case of manuscript transcription, a machine is first given images of a manuscript (input) alongside corresponding digital transcriptions (output); the machine next “trains” on the images and transcriptions and “learns” the transcription rules on its own. The machine then produces its own transcriptions on new, previously unseen images.

If automatic transcription reaches high enough accuracy rates, it could have a significant impact on medieval studies. Human transcription is expensive and enormously time-consuming: it requires a trained paleographer and hours of labor per manuscript. OCR would produce quick  Just as OCR enables keyword searches of printed texts on Google Books, so it could enable keyword searches of manuscripts. 

Producing transcriptions for a large corpus of medieval manuscripts would serve additional purposes beyond the digital facsimile experience. It would allow us to compare manuscripts across a single textual tradition, create digital editions for lesser-known texts, and conduct large-scale data analysis (such as topic modelling and text mining) across historical documents. Projects like the Piers Plowman Electronic Archive, which has produced digital transcriptions of eight B-Text manuscripts, attest to the usefulness of an easily accessible, readable, and usable digital archive. However,

human error is variable and unpredictable, whereas machine error is more likelier to be consistent, and when identified, systematically correctable.


Medieval manuscripts pose significant challenges to automated processing. Unlike printed texts, medieval handwriting often contains non-discrete characters, for example in the conjoined letters of cursive scripts or the disconnected minims of Gothic script; therefore machines cannot simply be taught individual letter forms, but must learn to transcribe larger segments of text [does the vatican project challenge this idea?]. Compared to modern handwritten documents, medieval manuscripts feature elaborate and often cryptic handwriting systems which vary intensely across period, region, and scribe. Accordingly, distinct OCR systems must be trained on distinct medieval scripts, which means compiling training data on tens, if not hundreds, of manuscripts per script. Furthermore, medieval manuscripts often have extensive marginal notes, decorations, and damage, making it difficult to distinguish text from other information in a digital image. Finally, given that the most advanced OCR engines often rely on comprehensive dictionaries and natural language processing to improve accuracy, the inconsistent orthography of medieval languages (like Middle English or Old French) poses an additional obstacle to efficient automated transcription.
