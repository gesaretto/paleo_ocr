---
title: "Conference Paper: Leeds 2019"
author: J. Schoen & G. E. Saretto
date: May 2019
---

- Sometime between roughly 1331 and 1341 a scribe in Ludlow compiled a collection of Middle English, Anglo-Norman, and Latin texts, with both prose and verse works that ranged from somber religious allegory to bawdy fabliau. The scribe then attached his miscellaneous folios to two older booklets of Anglo-Norman religious writing, and formed the manuscript now known as Harley MS 2253, which is held in the British Library.

- Harley MS 2253 is a decidedly multilingual manuscript and an unusually rich example of such a kind produced commonly in the 13th and 14th century. The polyglot manuscript is fertile ground for comparative literary analysis. But in the 20th century, the English, French, and Latin texts were never edited alongside each other. Instead, the Middle English texts were edited separately, and the Latin and Anglo-Norman texts were largely ignored.

- The Middle English lyrics were given special attention, and although the manuscript actually features more Anglo-Norman poems, "Harley Lyrics" became shorthand for the Middle English lyrics alone.

- Thus a distinctly multilingual manuscript was for decades edited and studied almost exclusively for its monolingual holdings, while little attention was given to its larger structural, thematic, and cross-linguistic implications.

- Significant work has been done in the past twenty years to correct this initial editorial bias, including a momentous edition of the entire manuscript. But the editorial history of Harley MS 2253 demonstrates the benefits and compromises behind any edition of a manuscript.

- Such advantages and constraints are inherent to the process of shifting manuscript to print, and indeed inherent to any form of remediation—that is, to cite Bolter and Grusin, "the representation of one medium in another."

- Remediation always affords certain conveniences--for instance, legibility, access, thematic considerations--at the expense of certain information--for instance, page layout, material, handwriting, structure. It is foremost essential to be aware of the shifting balance between information and convenience behind any remediation.

- The digital era has ushered in a whole new set of mediums for representing manuscripts: digital facsimiles, digital editions, isolated images of manuscript pages and illustrations. As with print, these new mediums raise questions about how to best represent manuscripts across different contexts and for different purposes. But the digital era is also introducing new processes of remediation—that is, changing not only the medium through which we reproduce manuscripts, but the method by which we do so.

- Editors can consult digital images when working on manuscripts; the internet allows scholars to collaborate more easily on editions; some research teams are even crowdsourcing transcriptions.  

- And the next form of remediation may well entail machines automatically transcribing manuscripts themselves.

- Thanks to advancements in Optical Character Recognition (OCR), several research teams (including our own) are successfully beginning to train machines to transcribe medieval scripts.

- In this talk we will tell you about an experiment with Optical Character Recognition that we started about a year and a half ago, with the scientific support of Columbia's Group for Experimental Methods in Humanistic Research and the financial support of Columbia's Data Science Institute. The experiment itself consisted in using a preexistent OCR software--called _Kraken_, like Tennyson's giant squid--to train a model for the automatic recognition and transcription of a Middle English book hand from the early 15th century.

- As you probably already know, Optical Character Recognition (OCR) is a promising technology that transforms a text-image--for example, a scanned book or a photograph of a document--into machine-readable text. That text can then be digitally searched, edited, stored, and otherwise manipulated by a computer. OCR technology already underlies many of the digital texts we read: it is what enables Project Gutenberg to easily digitize printed texts and Google Books to perform keyword-searches on scanned books.

- OCR models are built through machine learning. This means that the machine transcribes a text by following inferred rules rather than explicit instructions. More specifically, the machine learns these inferred rules through a "training" process, wherein a researcher provides the machine with a "training set" of paired inputs and outputs.

- To use manuscript transcription as an example, a machine is first given a training set composed of manuscript images--the input--alongside corresponding human-made transcriptions--the output. Then, the machine “trains” on the images and transcriptions and “learns” the transcription rules on its own, through guessing and correcting. The machine can then produce its own transcriptions on new, previously unseen images.

- The automation involved in this process ...



In this paper, we discuss our method for training an OCR system as well as our promising results. But we also discuss important considerations for any team embarking on an automatic transcription project, as well as what this experiment may teach us about manuscript studies in general.

- We decided to try OCR on a small corpus of Middle English manuscripts. To begin with, we needed to find a suitable OCR system to use. We decided on Kraken, which is a turn-key OCR suite developed and maintained by Benjamin Kiessling.

- Kraken has two primary benefits: first, it is open software. This means that any scholar can not only use it freely, but also figure out how it works, and--depending on one's technological prowess--alter it according to one's specific needs. Second, _Kraken_ is designed for connected scripts, like Arabic. This means that it learns to recognize writing not letter by letter, but line by line. As a consequence, we assumed that it would perform well on 14th and 15th-century book-hands, often characterized by biting, ligatures, and thick sequences of minims.

- After selecting our OCR system, we needed to selected our training manuscript--which is, again, the manuscript on which the machine would "learn" its rules for transcription. We selected Oxford _Corpus Christi 198_, a manuscript of the *Canterbury Tales* that was copied between 1410 and 1420. Its scribe, known as "Scribe D" among paleographers, is one of the most prolific scribes who worked in England between the late 14th century and the early 15th century.

- "Scribe D" writes in particularly neat version of a script called "Anglicana Formata." Because it draws letter forms from both formal and cursive scripts, "Anglicana Formata" poses similar challenges to automatic transcription as other medieval hands. For example, it features both the loops and slants of documentary and the minim compression of Gothic. Moreover, it is now widely assumed that "Scribe D" worked in the same professional circle as several other prominent London scribes. Thus, a model based on Scribe D could be also tested on the writing of other scribes who worked within his professional circle. Finally, and perhaps most importantly, we were able to download high-resolution images of MS 198 via the _Early Manuscripts at Oxford University_ project--a site that is now defunct.

- Having chosen a suitable manuscript and page range for our training set, we then proceeded to produce our own transcriptions of the manuscript images. Our training data amounted to about 1,330 lines, distributed across 40 manuscript pages.

- We believe that the choices that went into this particular step of the process deserve great attention. Not only did the preparation of these manual transcriptions ultimately constitute the bulk of our work, but it also posed the largest number of challenges.

- This is because Kraken—like most contemporary OCR systems—requires “diplomatic transcriptions.” In other words, transcriptions where every letter form represented on the page corresponds to a single character—or glyph—of plain text, in a strict one-to-one correlation. In paleography, the term “diplomatic transcription” similarly refers to transcriptions which render the text of a manuscript into accurate glyphs. But, in practice, most transcriptions are actually “semi-diplomatic.” For example, most medieval transcription guidelines recommend to expand abbreviations by including the omitted text in italics or parentheses. Therefore, even though a print transcription of our training manuscript does exist, its systematic expansion of abbreviations prevented us from using it.

- As a consequence, because we needed to render abbreviations into single Unicode characters, we had to assemble our 1,330 lines of training data from scratch, together with some 1,400 additional lines which we transcribed for testing purposes.

Presumably because semi-diplomatic transcriptions are the paleographic standard, there is no convention for rendering medieval abbreviations into Unicode characters. In other words, since print and digital transcriptions by and large expand abbreviations, there has been no need to create standards for digitally rendering abbreviations. We therefore had to create our own guideline for digital abbreviations, which required categorizing them and assigning them systematized Unicode characters (for example, rendering the “er” loop abbreviation with the ◌̉ diacritic, as in d̉). We decided to limit our selection of characters according to those proposed in the guidelines of the Medieval Unicode Font Initiative (MUFI). In particular, we followed MUFI’s recommendations to render less common characters and abbreviations through dependable Unicode encodings, thus ensuring that these could be displayed adequately across different systems and applications. Categorizing the abbreviations was a separate undertaking, given that there is no standard handbook for Middle English abbreviations (such as Cappelli serves for Latin paleography).

- There are also certain discrepancies among transcription systems which deserve particular attention as OCR technology improves. For example, paleographers have developed different systems for marking features such as erasures, corrections, and interlinear insertions. While these particular marks were not necessary for any of our transcriptions, they will certainly be necessary in future training sets. Moreover, a precise and consistent set of guidelines concerning how to prepare a transcription does not seem to exist yet, and these guidelines will be  required as we continue to train machines. 
For instance, should the machine learn the distinction between a long “s” (ſ) and a kidney-shaped “s” (ß)? Should it preserve the difference between an ampersand (&) and a tironian “et” (⁊)? Should it reproduce the contrast between a plain double “l” (ll) and a crossed—or “Welsh”—double "l" (ỻ)? Should it render a d with a loop abbreviation as “der” or “d̉”?If we are to successfully teach a machine to transcribe a manuscript, we need to agree on what a successful transcription looks like.

- Future software may be able to expand abbreviations in post-processing, and OCR technology may even be able to render semi-diplomatic transcriptions down the line. But the successful training of these models will still rest on the establishment of a shared and sustainable standard for transcriptions. Perhaps we will not only develop different models for different scripts, but also different models for different levels of transcription. Regardless, the imminent rise of OCR will compel scholars to reflect again on what features of a medieval script should be deemed essential to remediate and why.

- We fed _Kraken_ the images and our plain text transcriptions. Then, we ran the training program and waited, occasionally checking the results that appeared on screen over the course of six hours. We could witness the machine making attempts and correcting itself, from the strings of "Es" of the first trial to the somewhat recognizable words that appeared more and more often after the sixth. We stopped the program around its 36th cycle, and then tested the resulting model on _Corpus Christi 198_ itself and on four more contemporary manuscripts. On _Corpus Christi 198_, the OCR performed decently; its accuracy rate fluctuated between 90 and 85 percent. On _Harley 7334_, a different copy of _Canterbury Tales_ attributed to scribe D, it went down to about 80 percent. Then, on Scribe D's sections of the Trinity _Confessio Amantis_, it dropped to 70 percent. On _Trinity_ B.15.17, a copy of _Piers Plowman_ attributed to another scribe in his circle, it fell below 60 percent. Finally, it recognized less than 20 percent of the letter forms contained in Corpus Christi 201, a _Piers Plowman_ which features a far more cursive form of Anglicana. The decrease in these numbers should not come across as surprising. Of course, the more we depart from the single scribe, text, and script on which the model was trained, the less reliably this performs. Accordingly, future experiments could concentrate on training models that encompass multiple hands and variants.

- First, we would like to discuss the decisions behind manual transcriptions which are used to train the machine.

-

-
- Second, we would like to discuss the importance of open technology and data across medieval studies. Most importantly, it’s essential that all researchers working on automatic transcription projects make their training data and models freely available online. This way researchers can experiment with new OCR systems, collectively generate a large corpus of training manuscripts, and use trained OCR models for various transcription projects.

- Relatedly, training a transcription machine requires digital images of manuscripts. Luckily thousands of medieval manuscripts are fully digitized, and future digitization projects will provide more candidates for training and transcriptions. However, even though many manuscripts are digitized and available to view online in high-resolution, most libraries do not allow these images to be downloaded in high-resolution, which is required for the training process. Even in our relatively small experiment, we were extremely limited by the manuscript images freely available to download and those which libraries agreed to send us. While institutions may not want to make these images publically available, they will need to share them with researchers if they want a tool which will significantly improve their digital editions in the long run.

- [Maybe say something about open-software in general? Something about how doing projects like this can open our eyes to how restrictive the field can be?]

- In conclusion, we want to reiterate both the promising results of our experiment as well as the intriguing challenges ahead. We are excited by this technology's potential, and our experiment proved more successful than we imagined: but after working through this process, we have also come to realize that there critical conversations to be had about the [kinds of transcription we want this technology to produce, what a transcription is in the first place, and what kinds of biases underlie any transcription of a manuscript-- both in today's digital age and in the history of medieval studies]

- Rather than viewing automatic transcription as an extreme departure from the history of manuscript studies as we know it, we suggest understanding this moment like any other layer in a manuscript's remediation history. As with previous mediums, there are many potential benefits to automatic transcription, and there are also various constraints demanded by the process.

- Moreover, just as we argue for viewing current OCR projects as a discrete moment in manuscript history, so we suggest reading manuscript history through the lens of data science and machine learning. Returning to Harley MS 2253, we can think about this manuscript's editorial history in terms of data manipulation and remediation bias. In plucking the Middle English lyrics out of their original context and presenting them as a homogenous set, scholars mined the manuscript for a particular data set and discarded the rest. They manipulated the manuscript's data according to a certain bias—in favor of Middle English and discriminatory against Latin and Anglo-Norman. Even the facsimile edition—typically considered a rather transparent remediation—distorted data by leaving out the first two quires, reflecting a bias in favor of a single scribe as well as the Middle English. As in data science, these biases often have political and cultural roots: in privileging the Middle English lyrics scholars may have been engaged in a kind of medievalist nativism, implicitly marking Middle English as the “authentic” language of medieval England, and signalling a tie between nation and language when no such tie existed in medieval England. Of course, this is what all editions are—selections, curations, manipulations. Though Harley 2253's history may be exemplary, any medieval manuscript's editorial history is a history of data manipulation. But it is important to be aware of the ways in which we manipulate data and to consider the ramifications of our manipulation, especially in a technological age when decisions amplify rapidly and are often difficult to reverse.
